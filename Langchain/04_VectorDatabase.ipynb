{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8b3e1e",
   "metadata": {},
   "source": [
    "#### **What is a Vector Store in RAG?**\n",
    "After converting documents or queries into numerical vectors (embeddings), we need to:\n",
    "1. Store those vectors efficiently.\n",
    "2. Search for the most similar vectors quickly during retrieval.\n",
    "\n",
    "This is what a Vector Store (a.k.a. Vector Database) does.\n",
    "\n",
    "#### **Vector Store = (Embeddings + Metadata + Index)**\n",
    "A vector store contains:\n",
    "\n",
    "| Component    | Description                                                                 |\n",
    "| ------------ | --------------------------------------------------------------------------- |\n",
    "| **Vectors**  | Dense arrays representing documents (from embedding model)                  |\n",
    "| **Metadata** | Optional info like title, source, date, etc.                                |\n",
    "| **Index**    | A data structure for fast similarity search (like FAISS, Annoy, HNSW, etc.) |\n",
    "\n",
    "#### **How Vector Stores Work?**\n",
    "1. Indexing\n",
    "- After embedding, all document vectors are indexed using a fast approximate nearest neighbor (ANN) algorithm.\n",
    "- Popular ANN algorithms: FAISS, HNSW, Annoy, ScaNN.\n",
    "\n",
    "2. Similarity Search\n",
    "- At query time, your question is embedded → vector.\n",
    "- The store finds top-K most similar vectors to that query.\n",
    "- Similarity is computed using cosine similarity, dot product, or Euclidean distance.\n",
    "\n",
    "#### **Common Vector Stores in LangChain**\n",
    "\n",
    "| Vector Store     | Type        | Speed       | Offline? | Notes                               |\n",
    "| ---------------- | ----------- | ----------- | -------- | ----------------------------------- |\n",
    "| **FAISS**        | Local index | ⚡ Fast      | ✅ Yes    | Most used for learning & local apps |\n",
    "| **ChromaDB**     | Local + API | Medium      | ✅ Yes    | Lightweight DB with persistence     |\n",
    "| **Weaviate**     | Server DB   | Fast        | ❌/✅      | Graph-like queries, strong metadata |\n",
    "| **Pinecone**     | Cloud       | ⚡ Very fast | ❌        | Paid but powerful, scalable         |\n",
    "| **Qdrant**       | Cloud/local | Fast        | ✅ Yes    | Great with filters and metadata     |\n",
    "| **Milvus**       | Cloud/local | Fast        | ✅ Yes    | Used for large-scale prod search    |\n",
    "| **Redis Vector** | Hybrid DB   | Fast        | ✅ Yes    | Combines key-value with vectors     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06883b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 468, which is longer than the specified 300\n",
      "Created a chunk of size 387, which is longer than the specified 300\n",
      "Created a chunk of size 406, which is longer than the specified 300\n",
      "Created a chunk of size 331, which is longer than the specified 300\n",
      "Created a chunk of size 348, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 411, which is longer than the specified 300\n",
      "Created a chunk of size 303, which is longer than the specified 300\n",
      "Created a chunk of size 397, which is longer than the specified 300\n",
      "C:\\Users\\disha\\AppData\\Local\\Temp\\ipykernel_11484\\3047032281.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\disha\\miniconda3\\envs\\rag-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave-Particle Duality\n",
      "One of the most profound revelations of quantum mechanics is that particles, such as electrons and photons, exhibit both wave-like and particle-like properties. This dual nature was highlighted in the famous double-slit experiment, where particles produce an interference pattern typical of waves when not observed, but behave like discrete particles when measured.\n",
      "Quantum Tunneling\n",
      "Quantum tunneling is the phenomenon where particles pass through energy barriers higher than their kinetic energy. This effect is critical in nuclear fusion in stars and the operation of tunnel diodes and scanning tunneling microscopes.\n",
      "Quantum Mechanics: An Overview\n",
      "Quantum mechanics is a fundamental branch of physics that describes nature at the smallest scales of energy levels of atoms and subatomic particles. Unlike classical mechanics, where objects have definite positions and velocities, quantum mechanics introduces a probabilistic framework. Particles are described by wave functions, and these wave functions provide the probabilities of finding a particle in a particular location or state.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load and split\n",
    "loader = TextLoader(\"quantum.txt\")\n",
    "docs = loader.load()\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "# Save to disk\n",
    "vectorstore.save_local(\"my_faiss_index\")\n",
    "\n",
    "# Later: load and query\n",
    "loaded_vs = FAISS.load_local(\n",
    "    \"my_faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "docs = loaded_vs.similarity_search(\"Tell me about Greek letters\", k=3)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aeacaf",
   "metadata": {},
   "source": [
    "#### **Nearest Neighbor Search**:\n",
    "Given a query vector, nearest neighbor (NN) search aims to find the k vectors in a dataset that are most similar (nearest) to the query, according to a similarity measure.\n",
    "**Similarity Measures**:\n",
    "1. Cosine Similarity: Measures the cosine of the angle between vectors.\n",
    "2. Euclidean Distance: Measures straight-line distance in vector space.\n",
    "3. Dot Product / Inner Product: Common for normalized vectors (frequent in transformer embeddings).\n",
    "\n",
    "#### **Approximate Nearest Neighbor (ANN) Search**:\n",
    "ANN algorithms aim to approximate the nearest neighbors faster, especially in high-dimensional spaces (e.g., 768 or 1536 dimensions for BERT embeddings).\n",
    "\n",
    "##### **LSH (Locality Sensitive Hashing)**:\n",
    "Hash vectors into buckets such that similar vectors fall into the same bucket with high probability.\n",
    "**How**:\n",
    "- Use hash functions that preserve similarity.\n",
    "- Only compare the query with vectors in the same bucket.\n",
    "- Pros: Theoretically grounded, simple.\n",
    "- Cons: Struggles with very high-dimensional vectors, precision trade-offs.\n",
    "\n",
    "##### **Tree-Based Methods**:\n",
    "- **KD-Tree**: Good for: Low dimensions (<30). Splits the space hierarchically along axis-aligned hyperplanes. Not efficient in high-dimensions due to the curse of dimensionality.\n",
    "- **Ball Tree / Metric Tree**:Partitions the space using hyperspheres instead of hyperplanes. Better for some distance metrics.\n",
    "\n",
    "##### **Graph-Based Methods**:\n",
    "- **HNSW (Hierarchical Navigable Small World graphs)**: Currently the most popular ANN algorithm in vector DBs. Builds a multi-layer navigable small-world graph.Query: Starts from a random node and “navigates” to closer neighbors at each layer.Build time: High, but search is very fast and accurate. Used in: FAISS, Vespa, Weaviate, Qdrant. \n",
    "\n",
    "- **NSW (Navigable Small World)**: Earlier version of HNSW with a single-layer graph.\n",
    "\n",
    "- **Product Quantization (PQ) and IVF (Inverted File Index) — Used in FAISS**: IVF: Clusters the dataset and searches only in a few relevant clusters.PQ: Compresses vectors using quantization, enabling efficient memory usage. Combo IVF+PQ: Used in FAISS to balance speed, accuracy, and memory.\n",
    "\n",
    "- **ScaNN (Scalable Nearest Neighbors)**: Developed by Google. Uses asymmetric hashing, tree partitioning, and quantization. Highly optimized for TPUs and GPUs. Performs extremely well on large-scale datasets.\n",
    "\n",
    "\n",
    "##### **Similarity Searches in Vector Databases**\n",
    "When a query comes in:\n",
    "- It's converted to an embedding vector (via an LLM or sentence encoder).\n",
    "- That vector is passed to the vector DB.\n",
    "- The vector DB uses ANN to search for similar vectors.\n",
    "- The corresponding documents/chunks are retrieved and fed into the generation phase (e.g., with LLM).\n",
    "\n",
    "**Popular Vector DBs**:\n",
    "1. FAISS: Facebook’s library for vector search (CPU and GPU optimized).\n",
    "2. Pinecone: Fully managed, cloud-native vector DB.\n",
    "3. Weaviate: Feature-rich with built-in modules (text2vec, hybrid search).\n",
    "4. Qdrant: Focuses on high performance and filtering.\n",
    "5. Milvus: High-throughput vector database with scalability.\n",
    "6. Chroma: Simpler, Pythonic vector store (used in many RAG demos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792ae78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c28d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
