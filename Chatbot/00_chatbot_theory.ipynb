{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fa3fa0",
   "metadata": {},
   "source": [
    "#### **Key Components of Langchain Chatbot**:\n",
    "\n",
    "1. **LLM (Language Model)**: This is brain of chatbot. We can use LLMs like LLaMA2, Mixtral, or others served through Groq.\n",
    "2. **Prompt Template**: Defines how you ask the model something. LangChain supports structured prompts with variables (like {input}).\n",
    "3. **Chain**: Chains are sequences of components where the output of one becomes the input of the next. For chatbots, often this is LLMChain.\n",
    "4. **Memory**: Stores the conversation history. Types include ConversationBufferMemory, ConversationTokenBufferMemory, etc.\n",
    "5. **ChatMessageHistory**: Internal store of the back-and-forth messages between the user and the bot. Important for maintaining context.\n",
    "6. **Agent / Tool Use**: If your chatbot needs to perform external actions (like using a calculator or calling APIs), LangChain agents can be used.\n",
    "\n",
    "#### **Under the Hood: How transformers work in langchain**:\n",
    "1. Transformers like LLaMA or Mistral work by consuming input tokens and returning a sequence of output tokens.\n",
    "2. LangChain wraps these LLMs in a uniform API (LLM or ChatModel) and handles:\n",
    " - Prompt formatting.\n",
    " - Token counting.\n",
    " - Streaming response (if supported).\n",
    " - Memory injection (by appending message history to prompt)\n",
    "\n",
    "When using langchain_groq, you're using LLMs hosted on Groq servers with LangChain managing:\n",
    " 1. Input prompt (user message + memory context).\n",
    " 2. Sending to Groq-hosted LLM.\n",
    " 3. Receiving and displaying output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66a842",
   "metadata": {},
   "source": [
    "### Setup of Simple Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7592ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec8425bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "llm = ChatGroq(model= 'Gemma2-9b-It', groq_api_key= groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f500a046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: i am dishnat\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot:  Hello Dishnat! It's nice to meet you. What can I do for you today?  ðŸ˜Š  \n",
      "\n",
      "I'm eager to learn more about you! \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: i am dishnat\n",
      "AI: Hello Dishnat! It's nice to meet you. What can I do for you today?  ðŸ˜Š  \n",
      "\n",
      "I'm eager to learn more about you! \n",
      "\n",
      "\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot:  I'm excited to hear more about you, Dishnat!  \n",
      "\n",
      "Tell me, what are some of your interests?  Are you into reading, music, movies, or something else entirely?  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: i am dishnat\n",
      "AI: Hello Dishnat! It's nice to meet you. What can I do for you today?  ðŸ˜Š  \n",
      "\n",
      "I'm eager to learn more about you! \n",
      "\n",
      "\n",
      "Human: \n",
      "AI: I'm excited to hear more about you, Dishnat!  \n",
      "\n",
      "Tell me, what are some of your interests?  Are you into reading, music, movies, or something else entirely?  \n",
      "\n",
      "\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot:  That's great to hear, Dishnat!  \n",
      "\n",
      "Do you have a favorite book, song, or movie you'd like to tell me about? I'm always looking for new recommendations.  ðŸ“šðŸŽ¶ðŸŽ¬  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### adding memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "## create a conversion chain\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose= True\n",
    ")\n",
    "\n",
    "## chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "        break\n",
    "    response = conversation.predict(input = user_input)\n",
    "    print(\"Bot: \", response)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316473c2",
   "metadata": {},
   "source": [
    "1. https://jalammar.github.io/illustrated-transformer/\n",
    "2. https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "3. https://github.com/karpathy/nanoGPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50e8d4",
   "metadata": {},
   "source": [
    "1. https://python.langchain.com/docs/concepts/\n",
    "2. https://python.langchain.com/docs/modules/agents/\n",
    "3. https://github.com/ggml-org/llama.cpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a9c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
